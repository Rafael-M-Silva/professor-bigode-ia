/* o Next.js vai receber uma requisi√ß√£o HTTP POST.

voc√™ vai ler o corpo da requisi√ß√£o (payload).

verificar se o campo message existe.

se n√£o tiver, devolve um erro tipo ‚Äúmensagem obrigat√≥ria‚Äù.

üì¶ exemplo:

‚ÄúOl√°, tudo bem?‚Äù chega aqui como um objeto JSON. */

import { Ollama } from "ollama";
import { SYSTEM_PROMPT } from "../../rag/systemPrompt";
import { KNOWLEDGE } from "../../rag/knowledge";

export async function POST(request: Request) {
  const { message, history } = await request.json();

  console.log(history);

  if (!message) {
    return Response.json(
      { error: "Campo 'message' √© obrigat√≥rio" },
      { status: 400 }
    );
  }

  const origin = request.headers.get("origin") || "*";

  const corsHeaders = {
    "Access-Control-Allow-Origin": origin,
    "Access-Control-Allow-Methods": "POST, OPTIONS",
    "Access-Control-Headers": "Content-Type, Authorization",
  };

  if (request.method === "OPTIONS") {
    return new Response(null, {
      status: 204,
      headers: corsHeaders,
    });
  }

  const ollama = new Ollama({
    host: "https://ollama.com",
    headers: {
      Authorization: "Bearer " + process.env.OLLAMA_API_KEY,
    },
  });

  const response = await ollama.chat({
    model: "gpt-oss:120b",
    messages: [
      {
        role: "system",
        content:
         SYSTEM_PROMPT + "\n\n" + KNOWLEDGE
      },
      ...history,
    ],
    stream: false,
  });

  return Response.json(response.message.content);
}
